{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "evYH-h0dT9wC"
   },
   "source": [
    "   Copyright 2018 Google Inc.\n",
    "\n",
    "   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "   you may not use this file except in compliance with the License.\n",
    "   You may obtain a copy of the License at\n",
    "\n",
    "       http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "   Unless required by applicable law or agreed to in writing, software\n",
    "   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "   See the License for the specific language governing permissions and\n",
    "   limitations under the License.\n",
    "   \n",
    "   Author: Pramod Kaushik Mudrakarta (pramodkm@uchicago.edu)\n",
    "\n",
    "# Analysis of Neural Programmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of this notebook use code from: https://github.com/pramodkaushik/acl18_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RmyFSzJiT9wD",
    "outputId": "b2cd6236-3d35-4b41-aa6e-3ee51cdd2eb6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/pramodkm/tensorflow_gpu_python3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "from collections import Counter, defaultdict\n",
    "from random import shuffle\n",
    "import spacy\n",
    "import autoreload\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from IPython.display import HTML, Image, clear_output, display\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "sys.path.append('../neural_programmer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jFSazfXgT9wI"
   },
   "outputs": [],
   "source": [
    "import notebook_utils\n",
    "import data_utils\n",
    "from neural_programmer import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZiA8Ti1qT9wK"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NaTlbVCvT9wM"
   },
   "source": [
    "## Paths, parameters, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MyfxfPwhT9wM"
   },
   "outputs": [],
   "source": [
    "# Use only one GPU on the multi-GPU machine\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,3\"\n",
    "\n",
    "# WikiTableQuestions data\n",
    "DATA_DIR = '../wtq_data'\n",
    "PERTURBED_DATA_DIR = '../perturbed_wtq_data'\n",
    "\n",
    "# Pretrained model\n",
    "MODEL_FILE = os.path.join('..', 'pretrained_model', 'model_92500')\n",
    "model_step = int(MODEL_FILE.split('_')[-1])\n",
    "\n",
    "# Output directory to write attributions\n",
    "OUT_DIR = '../results'\n",
    "\n",
    "# Overstability curve file\n",
    "OVERSTABILITY_CURVE_FILE = os.path.join(OUT_DIR, 'overstability.eps')\n",
    "\n",
    "pd.options.display.max_colwidth=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lojEr5zhT9wO"
   },
   "source": [
    "### Some utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ZnjMUaiGT9wP"
   },
   "outputs": [],
   "source": [
    "# Operators whose results builds upon the result of previously applied operators\n",
    "acts_on_prev_result = {\n",
    "    'count': True,\n",
    "    'prev': True,\n",
    "    'next': True,\n",
    "    'first': True,\n",
    "    'last': True,\n",
    "    'mfe': False,\n",
    "    'greater': False,\n",
    "    'lesser': False,\n",
    "    'geq': False,\n",
    "    'leq': False,\n",
    "    'max': True,\n",
    "    'min': True,\n",
    "    'select': False,\n",
    "    'reset': False,\n",
    "    'print': True\n",
    "}\n",
    "\n",
    "# Operators whose result depends on the column it is acting on\n",
    "relies_on_col = {\n",
    "    'count': False,\n",
    "    'prev': False,\n",
    "    'next': False,\n",
    "    'first': False,\n",
    "    'last': False,\n",
    "    'mfe': True,\n",
    "    'greater': True,\n",
    "    'lesser': True,\n",
    "    'geq': True,\n",
    "    'leq': True,\n",
    "    'max': True,\n",
    "    'min': True,\n",
    "    'select': True,\n",
    "    'reset': False,\n",
    "    'print': True\n",
    "    \n",
    "}\n",
    "\n",
    "def get_program_mask(program, ignore_answer_cond=False):\n",
    "    \"\"\" \n",
    "    Returns a mask indicating attributions to which ops/cols are considered significant.\n",
    "    The conditions for are:\n",
    "    1) affect answer computation, (toggled by \"ignore_answer_cond\")\n",
    "    2) are not the same as their table-specific default counterparts\n",
    "    \n",
    "    program = [op (default_op), col (default_col)] * 4 \n",
    "    \"\"\"\n",
    "    mask = [False] * (2 * 4)\n",
    "    for i in range(3, -1, -1):\n",
    "        op, default_op = program[2*i].split('(')\n",
    "        op = op.strip()\n",
    "        default_op = default_op.strip().strip(')')\n",
    "        mask[2*i] = (op != default_op)\n",
    "        col, default_col = program[2*i + 1].split('(')\n",
    "        col = col.strip()\n",
    "        default_col = default_col.strip().strip(')')\n",
    "        if ignore_answer_cond:\n",
    "            continue\n",
    "        if relies_on_col[op]:\n",
    "            mask[2*i+1] = (col != default_col)\n",
    "        if not acts_on_prev_result[op]:\n",
    "            break\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uu6P34rkT9wR"
   },
   "source": [
    "## Load data, build graph and restore pretrained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ycnzxlorT9wR",
    "outputId": "1ecb02d7-a690-4617-d3a6-415bf242baf8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Annotated examples loaded ', 14152)\n",
      "('Annotated examples loaded ', 4344)\n",
      "('entry match token: ', 9133, 9133)\n",
      "('entry match token: ', 9134, 9134)\n",
      "('# train examples ', 10178)\n",
      "('# dev examples ', 2546)\n",
      "('# test examples ', 3913)\n"
     ]
    }
   ],
   "source": [
    "train_data, dev_data, test_data, utility, unprocessed_dev_data = notebook_utils.init_data(DATA_DIR)\n",
    "num_dev_examples = 2831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ubHggrz3T9wU",
    "outputId": "a500534e-ea31-4994-93ab-970bbf782092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forget gate bias\n",
      "('step: ', 0)\n",
      "('step: ', 1)\n",
      "('step: ', 2)\n",
      "('step: ', 3)\n",
      "('optimize params ', ['unit', 'word', 'word_match_feature_column_name', 'controller', 'column_controller', 'column_controller_prev', 'controller_prev', 'question_lstm_ix', 'question_lstm_fx', 'question_lstm_cx', 'question_lstm_ox', 'question_lstm_im', 'question_lstm_fm', 'question_lstm_cm', 'question_lstm_om', 'question_lstm_i', 'question_lstm_f', 'question_lstm_c', 'question_lstm_o', 'history_recurrent', 'history_recurrent_bias', 'break_conditional'])\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_grad/mul:0' shape=(15, 256) dtype=float64>, 'unit')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_1_grad/mul:0' shape=(10800, 256) dtype=float64>, 'word')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_2_grad/mul:0' shape=(1,) dtype=float64>, 'word_match_feature_column_name')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_3_grad/mul:0' shape=(512, 256) dtype=float64>, 'controller')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_4_grad/mul:0' shape=(512, 256) dtype=float64>, 'column_controller')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_5_grad/mul:0' shape=(256, 256) dtype=float64>, 'column_controller_prev')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_6_grad/mul:0' shape=(256, 256) dtype=float64>, 'controller_prev')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_7_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_ix')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_8_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_fx')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_9_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_cx')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_10_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_ox')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_11_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_im')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_12_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_fm')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_13_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_cm')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_14_grad/mul:0' shape=(256, 256) dtype=float64>, 'question_lstm_om')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_15_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_i')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_16_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_f')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_17_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_c')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_18_grad/mul:0' shape=(256,) dtype=float64>, 'question_lstm_o')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_19_grad/mul:0' shape=(768, 256) dtype=float64>, 'history_recurrent')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_20_grad/mul:0' shape=(1, 256) dtype=float64>, 'history_recurrent_bias')\n",
      "('grads: ', <tf.Tensor 'gradients_40/L2Loss_21_grad/mul:0' shape=(512, 256) dtype=float64>, 'break_conditional')\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess, graph, params = notebook_utils.build_graph(utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "5kpTozMRT9wX",
    "outputId": "390f9bc7-4c9d-46b8-d2d6-10230b2d0bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../pretrained_model/model_92500\n"
     ]
    }
   ],
   "source": [
    "sess, graph = notebook_utils.restore_model(sess, graph, params, MODEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UoaffUyHT9wa",
    "outputId": "22c0729d-df29-4057-def8-dd0c5799a958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dev set accuracy   after ', 92500, ' : ', 0.37195600942655144)\n",
      "(2546, 2546)\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "num_correct, num_examples, correct_dict = evaluate(sess, dev_data, utility.FLAGS.batch_size, graph, model_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "runhb7nbT9wc",
    "outputId": "f156b1a9-dbd1-4e87-962f-3466573c58c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.3345107735782409\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation accuracy:\", num_correct/float(num_dev_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYF6uWwXT9wf"
   },
   "source": [
    "## Apply Integrated Gradients (IG) \n",
    "Note: attributions are available already in \"results\". Hence, to reproduce the results of the ACL paper, one can skip this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "M6sPYsayT9wg"
   },
   "outputs": [],
   "source": [
    "# write attributions to this folder\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "if not os.path.isdir(attrs_outdir):\n",
    "    os.makedirs(attrs_outdir)\n",
    "\n",
    "# get embedding of dummy token\n",
    "embeddings = graph.params[\"word\"].eval()\n",
    "dummy_embedding = embeddings[utility.dummy_token_id, :]\n",
    "\n",
    "# which data to use?\n",
    "data = dev_data\n",
    "\n",
    "# number of sample points for Riemann integral computation\n",
    "num_points = 2000\n",
    "\n",
    "# hard coded stuff in the code\n",
    "question_attention_mask_value = -10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ydR-rH0nT9wi"
   },
   "outputs": [],
   "source": [
    "batch_size = graph.batch_size\n",
    "\n",
    "for offset in range(0, len(data) - graph.batch_size + 1, graph.batch_size):\n",
    "    feed_dict = data_utils.generate_feed_dict(data, offset, graph.batch_size, graph)\n",
    "\n",
    "    # first run inference to get operator and column sequences, and embeddings of question words\n",
    "    fetches = [graph.final_correct_list, graph.final_operation_softmax,\n",
    "               graph.final_column_softmax, graph.question_words_embeddings]\n",
    "    correct_list, operation_softmax, column_softmax, question_words_embeddings = sess.run(\n",
    "        fetches, feed_dict)\n",
    "\n",
    "    # compute table-specific default programs for tables in this batch\n",
    "    feed_copy = feed_dict.copy()\n",
    "    for t in graph.question_words_embeddings:\n",
    "        feed_copy[t] = np.concatenate(\n",
    "            [np.expand_dims(dummy_embedding, 0)]*batch_size, 0)\n",
    "\n",
    "    # Ideally the following line should be uncommented, but for attributions,\n",
    "    # we choose to keep this variable fixed. Note that this induces some bias\n",
    "    # in the attributions as the baseline is no longer an \"empty\" question, but\n",
    "    # an empty question where the question length is implicitly encoded in this variable\n",
    "    # feed_copy[graph.batch_question_attention_mask].fill(question_attention_mask_value)\n",
    "\n",
    "    feed_copy[graph.batch_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_exact_match])\n",
    "    feed_copy[graph.batch_column_exact_match] = np.zeros_like(\n",
    "        feed_copy[graph.batch_column_exact_match])\n",
    "\n",
    "    fetches = [graph.final_operation_softmax, graph.final_column_softmax]\n",
    "\n",
    "    default_operation_softmax, default_column_softmax = sess.run(\n",
    "        fetches, feed_copy)\n",
    "\n",
    "    for batch_id in range(batch_size):\n",
    "        wiki_example = data[offset+batch_id]\n",
    "\n",
    "        # get operator indices\n",
    "        op_indices = np.argmax(operation_softmax[batch_id, :, :], axis=1)\n",
    "        col_indices = np.argmax(column_softmax[batch_id, :, :], axis=1)\n",
    "\n",
    "        op_list = notebook_utils.softmax_to_names(\n",
    "            operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        col_list = notebook_utils.softmax_to_names(\n",
    "            column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        default_op_list = notebook_utils.softmax_to_names(\n",
    "            default_operation_softmax[batch_id, :, :], utility.operations_set)\n",
    "        default_col_list = notebook_utils.softmax_to_names(\n",
    "            default_column_softmax[batch_id, :, :], notebook_utils.get_column_names(wiki_example))\n",
    "\n",
    "        print([notebook_utils.rename(w) for w in op_list])\n",
    "        print(col_list)\n",
    "\n",
    "        # Sample points along the integral path and collect them as one batch\n",
    "        scaled_feed = feed_dict.copy()\n",
    "        for key in list(scaled_feed.keys()):\n",
    "            value = feed_dict[key]\n",
    "            if key.shape[0] == batch_size:  # this is a hack\n",
    "                scaled_feed[key] = [value[batch_id] for i in range(batch_size)]\n",
    "        scaled_feed[graph.op_ids] = op_indices\n",
    "        scaled_feed[graph.col_ids] = col_indices\n",
    "\n",
    "        num_examples = batch_size * int(num_points/float(batch_size))\n",
    "        scale = 1.0/num_examples\n",
    "\n",
    "        batch_op_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "        batch_col_attribution = np.zeros(\n",
    "            [graph.max_passes, graph.question_length+2], dtype=np.float32)\n",
    "\n",
    "        attr_op_softmax = []\n",
    "        attr_col_softmax = []\n",
    "\n",
    "        actual_num_numeric_cols = len(wiki_example.original_nc_names)\n",
    "        actual_num_word_cols = len(wiki_example.original_wc_names)\n",
    "\n",
    "        exact_match = wiki_example.exact_match\n",
    "        exact_column_match = wiki_example.exact_column_match\n",
    "\n",
    "        batch_question_embeddings = np.array(question_words_embeddings)[\n",
    "            :, batch_id, :]  # shape: 62 x 256\n",
    "\n",
    "        # split up set of points into batch_size'd batches\n",
    "        for k in range(0, num_examples, batch_size):\n",
    "            print('k:', k)\n",
    "            # scale question words to points between dummy_embedding and actual embedding\n",
    "            qw_jump = [None]*graph.question_length\n",
    "            for i, t in enumerate(graph.question_words_embeddings):\n",
    "                qw_jump[i] = scale * \\\n",
    "                    (batch_question_embeddings[i] - dummy_embedding)\n",
    "                scaled_feed[t] = [dummy_embedding + j*qw_jump[i]\n",
    "                                  for j in range(k, k+batch_size)]\n",
    "\n",
    "            # scale batch_exact_match\n",
    "            scaled_exact_match = []\n",
    "            scaled_column_exact_match = []\n",
    "\n",
    "            exact_match_jump = [None]*(graph.num_cols + graph.num_word_cols)\n",
    "            exact_column_match_jump = [None] * \\\n",
    "                (graph.num_cols + graph.num_word_cols)\n",
    "            for i in range(graph.num_cols):\n",
    "                if i < actual_num_numeric_cols:  # do not scale dummy columns\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = scale*np.array(exact_match[i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = scale * \\\n",
    "                        np.array(exact_column_match[i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[i] = 0\n",
    "\n",
    "            for i in range(graph.num_word_cols):\n",
    "                if i < actual_num_word_cols:  # do not scale dummy column names\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_match[graph.num_cols+i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_match[graph.num_cols+i])\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [j*scale*np.array(exact_column_match[graph.num_cols + i]) for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = scale * \\\n",
    "                        np.array(exact_column_match[graph.num_cols + i])\n",
    "                else:\n",
    "                    scaled_exact_match.append(np.expand_dims(\n",
    "                        [exact_match[graph.num_cols+i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_match_jump[graph.num_cols + i] = 0\n",
    "                    scaled_column_exact_match.append(np.expand_dims(\n",
    "                        [exact_column_match[graph.num_cols + i] for j in range(k, k+batch_size)], 1))\n",
    "                    exact_column_match_jump[graph.num_cols + i] = 0\n",
    "\n",
    "            scaled_feed[graph.batch_exact_match] = np.concatenate(\n",
    "                scaled_exact_match, 1)  # shape 20 x 40 x 100\n",
    "            scaled_feed[graph.batch_column_exact_match] = np.concatenate(\n",
    "                scaled_column_exact_match, 1)  # shape 20 x 40\n",
    "\n",
    "            # compute gradients\n",
    "            fetches = [graph.final_operation_softmax, graph.final_column_softmax, graph.operator_gradients,\n",
    "                       graph.column_gradients]\n",
    "            temp_op_softmax, temp_col_softmax, operator_gradients, column_gradients = sess.run(\n",
    "                fetches, scaled_feed)  # operator gradient shape: 4 x 62 x 20 x 256\n",
    "\n",
    "            attr_op_softmax.append(temp_op_softmax)\n",
    "            attr_col_softmax.append(temp_col_softmax)\n",
    "\n",
    "            # compute attributions\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(operator_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(operator_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([operator_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([operator_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_op_attribution[stage, :] += temp\n",
    "\n",
    "            for stage in range(graph.max_passes):\n",
    "                n = int(len(column_gradients)/graph.max_passes)\n",
    "                temp = [np.sum(column_gradients[n*stage][i]*qw_jump[i], axis=(0, 1))\n",
    "                        for i in range(graph.question_length)]\n",
    "                temp += [np.sum([column_gradients[n*stage+1][0][:, i, :]*exact_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                temp += [np.sum([column_gradients[n*stage+2][0][:, i]*exact_column_match_jump[i]\n",
    "                                 for i in range(graph.num_cols + graph.num_word_cols)])]\n",
    "                batch_col_attribution[stage, :] += temp\n",
    "\n",
    "        # sanity check to make sure the integral summation adds up to function difference\n",
    "        attr_op_softmax = np.concatenate(attr_op_softmax, axis=0)\n",
    "        attr_col_softmax = np.concatenate(attr_col_softmax, axis=0)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_op_attribution[stage, :])\n",
    "            input_fn_value = operation_softmax[batch_id,\n",
    "                                               stage, op_indices[stage]]\n",
    "            baseline_fn_value = attr_op_softmax[0, stage, op_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('OP', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "        for stage in range(graph.max_passes):\n",
    "            lhs = np.sum(batch_col_attribution[stage, :])\n",
    "            input_fn_value = column_softmax[batch_id,\n",
    "                                            stage, col_indices[stage]]\n",
    "            baseline_fn_value = attr_col_softmax[0, stage, col_indices[stage]]\n",
    "            rhs = input_fn_value - baseline_fn_value\n",
    "            print('COL', stage, ':', 'baseline=', baseline_fn_value, ', input_fn=',\n",
    "                  input_fn_value, 'check: ', lhs, ' - ', rhs, ' = ', lhs-rhs)\n",
    "\n",
    "        op_attributions = [None]*graph.max_passes\n",
    "        question_begin = np.nonzero(\n",
    "            wiki_example.question_attention_mask)[0].shape[0]\n",
    "\n",
    "        attributions_matrix = np.zeros(\n",
    "            [graph.question_length - question_begin + 2, 2 * graph.max_passes])\n",
    "        row_labels = []  # question words, tm, cm\n",
    "        col_labels = []  # operator and column selections\n",
    "        col_label_softmaxes = []  # softmaxes of the selections\n",
    "\n",
    "        for ix in range(question_begin, graph.question_length):\n",
    "            word = utility.reverse_word_ids[wiki_example.question[ix]]\n",
    "            if word == utility.unk_token:\n",
    "                word = word + '-' + [str(w) for w in wiki_example.string_question if w !=\n",
    "                                     wiki_example.question_number and w != wiki_example.question_number_1][ix - question_begin]\n",
    "            word = notebook_utils.rename(word)\n",
    "            row_labels.append(word)\n",
    "        row_labels.extend(['tm', 'cm'])\n",
    "\n",
    "        for stage in range(graph.max_passes):\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                op_list[stage]) + ' (' + notebook_utils.rename(default_op_list[stage]) + ')')\n",
    "            col_labels.append(notebook_utils.rename(\n",
    "                col_list[stage]) + ' (' + notebook_utils.rename(default_col_list[stage]) + ')')\n",
    "\n",
    "            col_label_softmaxes.append(str(operation_softmax[batch_id, stage, op_indices[stage]]) + ' (' + str(\n",
    "                default_operation_softmax[batch_id, stage, op_indices[stage]]) + ')')\n",
    "            col_label_softmaxes.append(str(column_softmax[batch_id, stage, col_indices[stage]]) + ' (' + str(\n",
    "                default_column_softmax[batch_id, stage, col_indices[stage]]) + ')')\n",
    "\n",
    "            attributions_matrix[:, 2 * stage] = batch_op_attribution[stage, question_begin:]\n",
    "            attributions_matrix[:, 2 * stage +\n",
    "                                1] = batch_col_attribution[stage, question_begin:]\n",
    "\n",
    "        question_string = ' '.join([notebook_utils.rename(str(w))\n",
    "                                    for w in wiki_example.string_question])\n",
    "\n",
    "        # save operator and column selections to file\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv'), 'w') as outf:\n",
    "            outf.write(question_string)\n",
    "            outf.write('\\n')\n",
    "            outf.write(str(correct_list[batch_id] == 1.0))\n",
    "            outf.write('\\n')\n",
    "            outf.write('\\t'.join(row_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_labels) + '\\n')\n",
    "            outf.write('\\t'.join(col_label_softmaxes) + '\\n')\n",
    "\n",
    "        # save attributions to file\n",
    "        np.savetxt(os.path.join(\n",
    "            attrs_outdir, wiki_example.question_id + '_attrs.txt'), attributions_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "WTfaO9HnT9wk"
   },
   "outputs": [],
   "source": [
    "data = dev_data\n",
    "attrs_outdir = os.path.join(OUT_DIR, 'attributions')\n",
    "figs_outdir = os.path.join(OUT_DIR, \"heatmaps\")\n",
    "if not os.path.isdir(figs_outdir):\n",
    "    os.makedirs(figs_outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "0HewmZv0T9wm"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=1.0)\n",
    "rc={'axes.labelsize': 11, 'xtick.labelsize': 14, 'ytick.labelsize': 14}\n",
    "sns.set(rc=rc)\n",
    "for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "    attributions = np.loadtxt(os.path.join(attrs_outdir, wiki_example.question_id + '_attrs.txt'))\n",
    "    with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "        lines = f.readlines()\n",
    "        xlabels = ['\\n'.join(w.split()) for w in lines[3].strip().split('\\t')]\n",
    "        ylabels = lines[2].strip().split('\\t')\n",
    "    mask = get_program_mask(lines[3].strip().split('\\t'))\n",
    "    mask = np.expand_dims(mask, 0)\n",
    "    plt.figure(figsize=(len(xlabels),len(ylabels)/2))\n",
    "    plot_data = attributions/attributions.sum(axis=0)*mask\n",
    "    with sns.axes_style('white'):\n",
    "        sns.heatmap(plot_data, cbar=False, xticklabels=xlabels, yticklabels=ylabels, annot=True, fmt='.2f', robust=True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figs_outdir, wiki_example.question_id + '.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "EAYKL_tVT9wo",
    "outputId": "9b3c443d-a6de-42ce-9c99-74a79f6a3876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizations written to ../results/visualizations.html\n"
     ]
    }
   ],
   "source": [
    "with tf.gfile.GFile(os.path.join(OUT_DIR, 'visualizations.html'), 'w') as htmlf:\n",
    "    html_str = '<head><link rel=\"stylesheet\" href=\"https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css\" integrity=\"sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm\" crossorigin=\"anonymous\"></head>'\n",
    "    html_str += '<body> <div class=\"container\"> <h3> Visualizations of the attributions for the Neural Programmer network <br> <small> Lighter colors indicate high values <br> Green and red questions indicate whether the network got the answer right (or wrong)</small></h3></div><br>'\n",
    "    html_str += '<div class=\"container\">'\n",
    "    for wiki_example in data[:graph.batch_size*int(len(data)/graph.batch_size)]:\n",
    "        with tf.gfile.GFile(os.path.join(attrs_outdir, wiki_example.question_id + '_labels.tsv')) as f:\n",
    "                lines = f.readlines()\n",
    "                html_str += wiki_example.question_id + ' <div class=' + ('\"text-success\"' if lines[1].strip() == 'True' else '\"text-danger\"') + '>' + lines[0] + '</div><br>'\n",
    "                html_str += '<img src=\"heatmaps/' + wiki_example.question_id + '.png\"></img><br><hr><br>'\n",
    "    \n",
    "    html_str += '</div></body></html>'\n",
    "    htmlf.write(html_str)\n",
    "    print(\"Visualizations written to\",os.path.join(OUT_DIR, 'visualizations.html'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "integrated_gradients.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
