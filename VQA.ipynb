{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdRVEKFjUr2X"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "sys.path.append(os.path.realpath('./pytorch-vqa'))\n",
    "sys.path.append(os.path.realpath('./pytorch-resnet'))\n",
    "\n",
    "import threading\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import resnet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "from model import Net, apply_attention, tile_2d_over_nd\n",
    "from utils import get_transform\n",
    "\n",
    "from captum.attr import IntegratedGradients, LayerConductance, Saliency, NoiseTunnel\n",
    "from captum.attr import InterpretableEmbeddingBase, TokenReferenceBase\n",
    "from captum.attr import visualization, configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.r_model = resnet.resnet152(pretrained=True)\n",
    "        self.r_model.eval()\n",
    "        self.r_model.to(device)\n",
    "\n",
    "        self.buffer = {}\n",
    "        lock = threading.Lock()\n",
    "\n",
    "        def save_output(module, input, output):\n",
    "            with lock:\n",
    "                self.buffer[output.device] = output\n",
    "\n",
    "        self.r_model.layer4.register_forward_hook(save_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.r_model(x)          \n",
    "        return self.buffer[x.device]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQA_Resnet_Model(Net):\n",
    "    def __init__(self, embedding_tokens):\n",
    "        super().__init__(embedding_tokens)\n",
    "        self.resnet_layer4 = ResNetLayer4()\n",
    "    \n",
    "    def forward(self, v, q, q_len):\n",
    "        q = self.text(q, list(q_len.data))\n",
    "        v = self.resnet_layer4(v)\n",
    "\n",
    "        v = v / (v.norm(p=2, dim=1, keepdim=True).expand_as(v) + 1e-8)\n",
    "\n",
    "        a = self.attention(v, q)\n",
    "        v = apply_attention(v, a)\n",
    "\n",
    "        combined = torch.cat([v, q], dim=1)\n",
    "        answer = self.classifier(combined)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_features(img):\n",
    "    img_transformed = transform(img)\n",
    "    img_batch = img_transformed.unsqueeze(0).to(device)\n",
    "    return img_batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_resnet_interpret_ig(image_filename, questions, targets):\n",
    "    img = Image.open(image_filename).convert('RGB')\n",
    "    original_image = transforms.Compose([transforms.Scale(int(image_size / central_fraction)),\n",
    "                                   transforms.CenterCrop(image_size), transforms.ToTensor()])(img) \n",
    "    \n",
    "    image_features = image_to_features(img).requires_grad_().to(device)\n",
    "    for question, target in zip(questions, targets):\n",
    "        q, q_len = encode_question(question)\n",
    "        q_input_embedding = interpretable_embedding.indices_to_embeddings(q).unsqueeze(0)\n",
    "\n",
    "        # Making prediction. The output of prediction will be visualized later\n",
    "        ans = vqa_resnet(image_features, q_input_embedding, q_len.unsqueeze(0))\n",
    "        pred, answer_idx = F.softmax(ans, dim=1).data.cpu().max(dim=1)\n",
    "\n",
    "        # generate reference for each sample\n",
    "        q_reference_indices = token_reference.generate_reference(q_len.item(), \n",
    "                                                                 device=device).unsqueeze(0)\n",
    "        q_reference = interpretable_embedding.indices_to_embeddings(q_reference_indices).to(device)\n",
    "        attributions = ig.attribute(inputs=(image_features, q_input_embedding),\n",
    "                                    baselines=(image_features * 0.0, q_reference),\n",
    "                                    target=answer_idx,\n",
    "                                    additional_forward_args=q_len.unsqueeze(0),\n",
    "                                    n_steps=30)\n",
    "        # Visualize text attributions\n",
    "        text_attributions_norm = attributions[1].sum(dim=2).squeeze(0).norm()\n",
    "        vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                attributions[1].sum(dim=2).squeeze(0) / text_attributions_norm,\n",
    "                                pred[0].item(),\n",
    "                                answer_words[ answer_idx ],\n",
    "                                answer_words[ answer_idx ],\n",
    "                                target,\n",
    "                                attributions[1].sum(),       \n",
    "                                question.split(),\n",
    "                                0.0)]\n",
    "        visualization.visualize_text(vis_data_records)\n",
    "\n",
    "        # visualize image attributions\n",
    "        original_im_mat = np.transpose(original_image.cpu().detach().numpy(), (1, 2, 0))\n",
    "        attr = np.transpose(attributions[0].squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "        \n",
    "        visualization.visualize_image_attr_multiple(attr, original_im_mat, \n",
    "                                                    [\"original_image\", \"heat_map\"], [\"all\", \"absolute_value\"], \n",
    "                                                    titles=[\"Original Image\", \"Attribution Magnitude\"],\n",
    "                                                    cmap=default_cmap,\n",
    "                                                    show_colorbar=True)\n",
    "        print('Text Contributions: ', attributions[1].sum().item())\n",
    "        print('Image Contributions: ', attributions[0].sum().item())\n",
    "        print('Total Contribution: ', attributions[0].sum().item() + attributions[1].sum().item())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_resnet_interpret_saliency(image_filename, questions, targets):\n",
    "    img = Image.open(image_filename).convert('RGB')\n",
    "    original_image = transforms.Compose([transforms.Scale(int(image_size / central_fraction)),\n",
    "                                   transforms.CenterCrop(image_size), transforms.ToTensor()])(img) \n",
    "    \n",
    "    image_features = image_to_features(img).requires_grad_().to(device)\n",
    "    for question, target in zip(questions, targets):\n",
    "        q, q_len = encode_question(question)\n",
    "        q_input_embedding = interpretable_embedding.indices_to_embeddings(q).unsqueeze(0)\n",
    "\n",
    "        # Making prediction. The output of prediction will be visualized later\n",
    "        ans = vqa_resnet(image_features, q_input_embedding, q_len.unsqueeze(0))\n",
    "        pred, answer_idx = F.softmax(ans, dim=1).data.cpu().max(dim=1)\n",
    "\n",
    "        # generate reference for each sample\n",
    "        q_reference_indices = token_reference.generate_reference(q_len.item(), \n",
    "                                                                 device=device).unsqueeze(0)\n",
    "        q_reference = interpretable_embedding.indices_to_embeddings(q_reference_indices).to(device)\n",
    "        attributions = saliency.attribute(inputs=(image_features, q_input_embedding),\n",
    "                                    baselines=(image_features * 0.0, q_reference),\n",
    "                                    target=answer_idx,\n",
    "                                    additional_forward_args=q_len.unsqueeze(0),\n",
    "                                    n_steps=30)\n",
    "        # Visualize text attributions\n",
    "        text_attributions_norm = attributions[1].sum(dim=2).squeeze(0).norm()\n",
    "        vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                attributions[1].sum(dim=2).squeeze(0) / text_attributions_norm,\n",
    "                                pred[0].item(),\n",
    "                                answer_words[ answer_idx ],\n",
    "                                answer_words[ answer_idx ],\n",
    "                                target,\n",
    "                                attributions[1].sum(),       \n",
    "                                question.split(),\n",
    "                                0.0)]\n",
    "        visualization.visualize_text(vis_data_records)\n",
    "\n",
    "        # visualize image attributions\n",
    "        original_im_mat = np.transpose(original_image.cpu().detach().numpy(), (1, 2, 0))\n",
    "        attr = np.transpose(attributions[0].squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "        \n",
    "        visualization.visualize_image_attr_multiple(attr, original_im_mat, \n",
    "                                                    [\"original_image\", \"heat_map\"], [\"all\", \"absolute_value\"], \n",
    "                                                    titles=[\"Original Image\", \"Attribution Magnitude\"],\n",
    "                                                    cmap=default_cmap,\n",
    "                                                    show_colorbar=True)\n",
    "        print('Text Contributions: ', attributions[1].sum().item())\n",
    "        print('Image Contributions: ', attributions[0].sum().item())\n",
    "        print('Total Contribution: ', attributions[0].sum().item() + attributions[1].sum().item())\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vqa_resnet_interpret_sg(image_filename, questions, targets):\n",
    "    img = Image.open(image_filename).convert('RGB')\n",
    "    original_image = transforms.Compose([transforms.Scale(int(image_size / central_fraction)),\n",
    "                                   transforms.CenterCrop(image_size), transforms.ToTensor()])(img) \n",
    "    \n",
    "    image_features = image_to_features(img).requires_grad_().to(device)\n",
    "    for question, target in zip(questions, targets):\n",
    "        q, q_len = encode_question(question)\n",
    "        q_input_embedding = interpretable_embedding.indices_to_embeddings(q).unsqueeze(0)\n",
    "\n",
    "        # Making prediction. The output of prediction will be visualized later\n",
    "        ans = vqa_resnet(image_features, q_input_embedding, q_len.unsqueeze(0))\n",
    "        pred, answer_idx = F.softmax(ans, dim=1).data.cpu().max(dim=1)\n",
    "\n",
    "        # generate reference for each sample\n",
    "        q_reference_indices = token_reference.generate_reference(q_len.item(), \n",
    "                                                                 device=device).unsqueeze(0)\n",
    "        q_reference = interpretable_embedding.indices_to_embeddings(q_reference_indices).to(device)\n",
    "        attributions = sg.attribute(inputs=(image_features, q_input_embedding),\n",
    "                                    target=answer_idx,\n",
    "                                    additional_forward_args=q_len.unsqueeze(0),\n",
    "                                    nt_type='smoothgrad',\n",
    "                                    n_samples=10)\n",
    "        # Visualize text attributions\n",
    "        text_attributions_norm = attributions[1].sum(dim=2).squeeze(0).norm()\n",
    "        vis_data_records = [visualization.VisualizationDataRecord(\n",
    "                                attributions[1].sum(dim=2).squeeze(0) / text_attributions_norm,\n",
    "                                pred[0].item(),\n",
    "                                answer_words[ answer_idx ],\n",
    "                                answer_words[ answer_idx ],\n",
    "                                target,\n",
    "                                attributions[1].sum(),       \n",
    "                                question.split(),\n",
    "                                0.0)]\n",
    "        visualization.visualize_text(vis_data_records)\n",
    "\n",
    "        # visualize image attributions\n",
    "        original_im_mat = np.transpose(original_image.cpu().detach().numpy(), (1, 2, 0))\n",
    "        attr = np.transpose(attributions[0].squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "        \n",
    "        visualization.visualize_image_attr_multiple(attr, original_im_mat, \n",
    "                                                    [\"original_image\", \"heat_map\"], [\"all\", \"absolute_value\"], \n",
    "                                                    titles=[\"Original Image\", \"Attribution Magnitude\"],\n",
    "                                                    cmap=default_cmap,\n",
    "                                                    show_colorbar=True)\n",
    "        print('Text Contributions: ', attributions[1].sum().item())\n",
    "        print('Image Contributions: ', attributions[0].sum().item())\n",
    "        print('Total Contribution: ', attributions[0].sum().item() + attributions[1].sum().item())\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H68N6nIfV2Gr"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oikjgR5BWQ57"
   },
   "outputs": [],
   "source": [
    "saved_state = torch.load('./2017-08-04_00.55.19.pth', map_location=device)\n",
    "\n",
    "vocab = saved_state['vocab']\n",
    "token_to_index = vocab['question']\n",
    "answer_to_index = vocab['answer']\n",
    "num_tokens = len(token_to_index) + 1\n",
    "\n",
    "answer_words = ['unk'] * len(answer_to_index)\n",
    "for w, idx in answer_to_index.items():\n",
    "    answer_words[idx]=w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "colab_type": "code",
    "id": "Y_9fcj8RWXNl",
    "outputId": "b669ac9a-354d-464b-c8e5-c819f221d5f7"
   },
   "outputs": [],
   "source": [
    "vqa_net = torch.nn.DataParallel(Net(num_tokens))\n",
    "vqa_net.load_state_dict(saved_state['weights'])\n",
    "vqa_net.to(device)\n",
    "vqa_net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uBtu26tDWae_"
   },
   "outputs": [],
   "source": [
    "def encode_question(question):\n",
    "    question_arr = question.lower().split()\n",
    "    vec = torch.zeros(len(question_arr), device=device).long()\n",
    "    for i, token in enumerate(question_arr):\n",
    "        index = token_to_index.get(token, 0)\n",
    "        vec[i] = index\n",
    "    return vec, torch.tensor(len(question_arr), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "eo_6unKCWfjV",
    "outputId": "7d4dea71-e072-4571-f316-1fda2e3a5893"
   },
   "outputs": [],
   "source": [
    "vqa_resnet = VQA_Resnet_Model(vqa_net.module.text.embedding.num_embeddings)\n",
    "vqa_resnet = torch.nn.DataParallel(vqa_resnet)\n",
    "partial_dict = vqa_net.state_dict()\n",
    "state = vqa_resnet.state_dict()\n",
    "state.update(partial_dict)\n",
    "vqa_resnet.load_state_dict(state)\n",
    "vqa_resnet.to(device)\n",
    "vqa_resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "3BUJa2EOWiZZ",
    "outputId": "f5bf95c8-6d43-450e-90c4-d1660c624239"
   },
   "outputs": [],
   "source": [
    "image_size = 448\n",
    "central_fraction = 1.0\n",
    "transform = get_transform(image_size, central_fraction=central_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "2OMnxt4XWnun",
    "outputId": "394eaa97-b754-448f-af37-ed90784a8190"
   },
   "outputs": [],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(vqa_resnet, 'module.text.embedding')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtiIrVZhWroO"
   },
   "outputs": [],
   "source": [
    "PAD_IND = token_to_index['pad']\n",
    "token_reference = TokenReferenceBase(reference_token_idx=PAD_IND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qsVpK43wXOVo"
   },
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geQQD0dWXPix"
   },
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(vqa_resnet)\n",
    "saliency = Saliency(vqa_resnet)\n",
    "nt = NoiseTunnel(ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UfZ1EZBmXQ6P"
   },
   "outputs": [],
   "source": [
    "default_cmap = LinearSegmentedColormap.from_list('custom blue', \n",
    "                                                 [(0, '#ffffff'),\n",
    "                                                  (0.25, '#252b36'),\n",
    "                                                  (1, '#000000')], N=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFJ9a7SvXSxd"
   },
   "outputs": [],
   "source": [
    "images = ['./siamese.jpg',\n",
    "          './captum/tutorials/img/vqa/elephant.jpg',\n",
    "          './captum/tutorials/img/vqa/zebra.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "colab_type": "code",
    "id": "8xkw11vuXWjO",
    "outputId": "9be4a3ac-af10-45ae-81d0-1f31ea84d3d4"
   },
   "outputs": [],
   "source": [
    "image_idx = 0 # cat\n",
    "\n",
    "vqa_resnet_interpret(images[image_idx], [\n",
    "    \"what is on the picture\",\n",
    "    \"what color are the cat's eyes\",\n",
    "    \"is the animal in the picture a cat or a fox\",\n",
    "    \"what color is the cat\",\n",
    "    \"how many ears does the cat have\",\n",
    "    \"where is the cat\"\n",
    "], ['cat', 'blue', 'cat', 'white and brown', '2', 'at the wall'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bn38MZrgYQM4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VQA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
